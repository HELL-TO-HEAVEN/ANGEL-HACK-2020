{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd, datetime\n",
    "import joblib\n",
    "import pickle\n",
    "import geopandas as gpd\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import radians\n",
    "import networkx as nx\n",
    "import lightgbm as lgb\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input all the pickles from the folder \"Additional Data & Models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 20)\n",
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geo = joblib.load( out + 'df_geo.pkl')\n",
    "G = joblib.load( out + 'G.pkl')\n",
    "weather_data_filter = joblib.load( out + 'weather_data_filter.pkl')\n",
    "model = joblib.load( out + 'model.pkl')\n",
    "\n",
    "destination_pivot_df = joblib.load( out + 'destination_pivot_df.pkl')\n",
    "pickup_pivot_df = joblib.load( out + 'pickup_pivot_df.pkl')\n",
    "\n",
    "h = joblib.load( out + 'h.pkl')\n",
    "a = joblib.load( out + 'a.pkl')\n",
    "inCentrality = joblib.load( out + 'inCentrality.pkl')\n",
    "outCentrality = joblib.load( out + 'outCentrality.pkl')\n",
    "loadCentrality = joblib.load( out + 'loadCentrality.pkl')\n",
    "\n",
    "loaded_model_1 = joblib.load( out + 'loaded_model_1.pkl')\n",
    "loaded_model_2 = joblib.load( out + 'loaded_model_2.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def getbaseLineETA(df,columns=[\"pickup_id\",\"destination_id\",\"trj_id\"]):\n",
    "    predictedTime=[]\n",
    "    predictedPath=[]\n",
    "    predictedPath2=[]\n",
    "    predictedTime2=[]\n",
    "    for item in df[columns].values.tolist():\n",
    "        if G.has_node(item[0]) and G.has_node(item[1]):\n",
    "            if nx.has_path(G,item[0],item[1]):\n",
    "                shortestPath=nx.dijkstra_path(G,item[0],item[1], weight=\"popularity+time\")\n",
    "                shortestPath2=nx.dijkstra_path(G,item[0],item[1], weight=\"mean\")\n",
    "                total_time=0\n",
    "                for index in range(len(shortestPath)-1):\n",
    "                    a=shortestPath[index]\n",
    "                    b=shortestPath[index+1]\n",
    "                    total_time+=G[a][b]['mean']\n",
    "                predictedTime.append(total_time)\n",
    "                predictedPath.append(shortestPath)\n",
    "                predictedTime2.append(nx.dijkstra_path_length(G,item[0],item[1], weight=\"mean\"))\n",
    "                predictedPath2.append(shortestPath2)\n",
    "            else:\n",
    "                predictedTime.append(\"\")\n",
    "                predictedPath.append([])\n",
    "                predictedTime2.append(\"\")\n",
    "                predictedPath2.append([])\n",
    "        else:\n",
    "            predictedTime.append(\"\")\n",
    "            predictedPath.append([])\n",
    "            predictedTime2.append(\"\")\n",
    "            predictedPath2.append([])\n",
    "    return predictedTime,predictedPath,predictedTime2,predictedPath2\n",
    "\n",
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns \n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)\n",
    "    \n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371 # Radius of earth in kilometers. Use 3956 for miles\n",
    "    return c * r\n",
    "\n",
    "def euclidean_distance(x,y):\n",
    " \n",
    "    return np.sqrt((np.sum(np.power(x-y,2),axis=1)))\n",
    "\n",
    "def manhattan_distance(x,y):\n",
    " \n",
    "    return np.sum(np.abs(x-y),axis=1)\n",
    "\n",
    "def classify(x):\n",
    "    if x >=8 and x<=9:\n",
    "        return 10\n",
    "    elif x>=17 and x<=19:\n",
    "        return 50\n",
    "    return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(df_json):\n",
    "    sample_test_data = pd.read_json(df_json)\n",
    "    sample_test_data[\"sequence\"]=sample_test_data.index\n",
    "    gdf_origin = gpd.GeoDataFrame(sample_test_data.copy(),   geometry=gpd.points_from_xy(sample_test_data.longitude_origin, sample_test_data.latitude_origin),crs={'init': 'epsg:4326'})\n",
    "    gdf_destination = gpd.GeoDataFrame(sample_test_data.copy(),   geometry=gpd.points_from_xy(sample_test_data.longitude_destination, sample_test_data.latitude_destination),crs={'init': 'epsg:4326'})  \n",
    "\n",
    "    sjoined_origin = gpd.sjoin(gdf_origin, df_geo, op=\"within\",how=\"left\")\n",
    "    sjoined_origin['id']=sjoined_origin['id'].fillna(-1)\n",
    "\n",
    "    sjoined_destination = gpd.sjoin(gdf_destination, df_geo, op=\"within\",how=\"left\")\n",
    "    sjoined_destination['id']=sjoined_destination['id'].fillna(-1)\n",
    "\n",
    "    sjoined_origin_selected= sjoined_origin[['latitude_origin','longitude_origin','timestamp','hour_of_day','day_of_week','id','REGION_N','sequence']]\n",
    "    sjoined_destination_selected= sjoined_destination[['latitude_destination','longitude_destination','id','REGION_N','sequence']]\n",
    "\n",
    "    sjoined_origin_selected.rename(columns={'id': 'pickup_id','REGION_N':'REGION_N_pickup'}, inplace=True)\n",
    "    sjoined_destination_selected.rename(columns={'id': 'destination_id','REGION_N':'REGION_N_dest'}, inplace=True)\n",
    "\n",
    "    data_joined= pd.merge(sjoined_origin_selected,sjoined_destination_selected, how='left', on='sequence')\n",
    "    data_joined['Date'] = pd.to_datetime(data_joined.timestamp,unit='s')\n",
    "    data_joined.rename(columns={'sequence': 'trj_id'}, inplace=True)\n",
    "\n",
    "    predictedTime,predictedPath,predictedTime2,predictedPath2 =getbaseLineETA(data_joined)\n",
    "    data_joined[\"popularity+time\"]=pd.Series(predictedTime)\n",
    "    data_joined[\"popularity+time_path\"]=pd.Series(predictedPath)\n",
    "    data_joined[\"timeOnly\"]=pd.Series(predictedTime2)\n",
    "    data_joined[\"timeOnlyPath\"]=pd.Series(predictedPath2)\n",
    "\n",
    "    data_joined[\"dayOfWeek\"]=data_joined[\"Date\"].dt.dayofweek\n",
    "    data_joined[\"Day\"]=data_joined[\"Date\"].dt.day\n",
    "    data_joined[\"month\"]=data_joined[\"Date\"].dt.month\n",
    "    data_joined[\"hour\"]=data_joined['Date'].dt.hour\n",
    "\n",
    "    data_joined[\"pathHIndex\"]=data_joined['popularity+time_path'].apply(lambda x:sum(map(lambda item:h[item],x)))\n",
    "    data_joined[\"pathaIndex\"]=data_joined['popularity+time_path'].apply(lambda x:sum(map(lambda item:a[item],x)))\n",
    "    data_joined[\"pathInDegreeIndex\"]=data_joined['popularity+time_path'].apply(lambda x:sum(map(lambda item:inCentrality[item],x)))\n",
    "    data_joined[\"pathOutDegreeIndex\"]=data_joined['popularity+time_path'].apply(lambda x:sum(map(lambda item:outCentrality[item],x)))\n",
    "    data_joined[\"CountOfGrid\"]=data_joined['popularity+time_path'].apply(len)\n",
    "    data_joined[\"destLoad\"]=data_joined.destination_id.map(loadCentrality)\n",
    "    data_joined[\"destHIndex\"]=data_joined.destination_id.map(h)\n",
    "    data_joined[\"pickUpLoad\"]=data_joined.pickup_id.map(loadCentrality)\n",
    "    data_joined[\"flowType\"]=data_joined.REGION_N_pickup +\"->\"+data_joined.REGION_N_dest\n",
    "\n",
    "    data_joined['Date_round'] = data_joined['Date'].dt.floor('h')\n",
    "\n",
    "    data_joined1= pd.merge(data_joined,weather_data_filter, how='left', left_on='Date_round', right_on='NewDT_round')\n",
    "    data_joined1['Temp'] = data_joined1['Temp'].str.replace('Â°F', '')\n",
    "    data_joined1['Weather'] = data_joined1['Weather'].str.replace('.', '')\n",
    "    data_joined1['Wind'] = data_joined1['Wind'].str.replace('mph', '')\n",
    "    data_joined1['Humidity'] = data_joined1['Humidity'].str.replace('%', '')\n",
    "    data_joined1['Barometer'] = data_joined1['Barometer'].str.replace('\"Hg', '')\n",
    "    data_joined1['Visibility'] = data_joined1['Visibility'].str.replace('mi', '')\n",
    "    data_joined1['Wind']=np.where((data_joined1['Wind'].isnull())|(data_joined1['Wind']=='No wind'),'0',data_joined1['Wind'] )\n",
    "    data_joined1['Visibility']=np.where((data_joined1['Visibility'].isnull())|(data_joined1['Visibility']=='N/A'),'0',data_joined1['Visibility'] )\n",
    "    data_joined1['Temp'] = data_joined1['Temp'].astype(np.float64)\n",
    "    data_joined1['Wind'] = data_joined1['Wind'].astype(np.float64)\n",
    "    data_joined1['Humidity'] = data_joined1['Humidity'].astype(np.float64)\n",
    "    data_joined1['Barometer'] = data_joined1['Barometer'].astype(np.float64)\n",
    "    data_joined1['Visibility'] = data_joined1['Visibility'].astype(np.float64)\n",
    "\n",
    "    df_join2= pd.merge(data_joined1, pickup_pivot_df, how='left', left_on='pickup_id', right_on='pickup_id_pick')\n",
    "    df_join3= pd.merge(df_join2,destination_pivot_df, how='left', left_on='destination_id', right_on='destination_id_des')\n",
    "    df_original=df_join3[[\"trj_id\",'latitude_origin','longitude_origin','latitude_destination','longitude_destination','timestamp','hour_of_day','day_of_week']]\n",
    "\n",
    "    df_join3[\"Festival\"]=np.where(df_join3['day'].isin(['2019-04-01','2019-04-18','2019-04-19','2019-04-20','2019-04-21','2019-05-01','2019-05-11','2019-05-12','2019-05-17','2019-05-18','2019-05-19','2019-05-20']),1,0)\n",
    "    df_geo[\"X\"]=df_geo.geometry.centroid.x\n",
    "    df_geo[\"y\"]=df_geo.geometry.centroid.y\n",
    "    df_join3=df_join3.merge(df_geo[[\"id\",\"X\",\"y\"]],left_on=\"pickup_id\",right_on=\"id\",how=\"left\").merge(df_geo[[\"id\",\"X\",\"y\"]],left_on=\"destination_id\",right_on=\"id\",suffixes=(\"_pickup\",\"_dest\"),how=\"left\")\n",
    "    df_join3[(df_join3['popularity+time'].astype(str).apply(lambda x: len(x)) == 0)|(df_join3['timeOnlyPath'].astype(str).apply(lambda x: len(x)) == 0)]\n",
    "    df_join3['rawlat_pickup']=df_join3['latitude_origin']\n",
    "    df_join3['rawlng_pickup']=df_join3['longitude_origin']\n",
    "    df_join3['rawlat_dest']=df_join3['latitude_destination']\n",
    "    df_join3['rawlng_dest']=df_join3['longitude_destination']\n",
    "    df_join3[\"rawlng_pickup2\"] = df_join3[\"rawlng_pickup\"].apply(radians)\n",
    "    df_join3[\"rawlat_pickup2\"] = df_join3[\"rawlat_pickup\"].apply(radians)\n",
    "    df_join3[\"rawlng_dest2\"] = df_join3[\"rawlng_dest\"].apply(radians)\n",
    "    df_join3[\"rawlat_dest2\"] = df_join3[\"rawlat_dest\"].apply(radians)\n",
    "    df_join3[\"HarvsineDistance\"]=haversine(df_join3[\"rawlng_pickup2\"], df_join3[\"rawlat_pickup2\"], df_join3[\"rawlng_dest2\"], df_join3[\"rawlat_dest2\"])\n",
    "    df_join3[\"EuclideanDistance\"]=euclidean_distance(df_join3[[\"rawlng_pickup\",\"rawlat_pickup\"]].values, df_join3[[\"rawlng_dest\",\"rawlat_dest\"]].values)\n",
    "    df_join3[\"ManhattanDistance\"]=manhattan_distance(df_join3[[\"rawlng_pickup\",\"rawlat_pickup\"]].values, df_join3[[\"rawlng_dest\",\"rawlat_dest\"]].values)\n",
    "    df_join3[\"TimeInterval\"]=df_join3.hour.apply(classify)\n",
    "    df_join3[\"PopularPlace\"]=0\n",
    "    df_join3[\"Weekend\"]=df_join3.dayOfWeek.map({6:1,5:1}).fillna(0)\n",
    "    df_join3.loc[df_join3.destination_id.isin([6815,6816,6885,6886,517,587,588,658,659,590,519,589,2877,2876,2875,2947,2946,2945]),'PopularPlace']=1\n",
    "\n",
    "    # can match\n",
    "    data_for_model_1=df_join3[~((df_join3['popularity+time'].astype(str).apply(lambda x: len(x)) == 0)|(df_join3['timeOnlyPath'].astype(str).apply(lambda x: len(x)) == 0))]\n",
    "    var_drop=['latitude_origin','longitude_origin','timestamp','pickup_id','REGION_N_pickup','destination_id','latitude_destination',\n",
    "         'longitude_destination','Date','popularity+time_path','timeOnlyPath','hour_of_day','day_of_week','month','Date_round',\n",
    "         'Time','day','Time24','NewDT','session','NewDT_round','pickup_id_pick','destination_id_des',\n",
    "         'id_pickup','X_pickup','y_pickup','id_dest','X_dest','y_dest','rawlng_pickup2','rawlat_pickup2','rawlng_dest2','rawlat_dest2',\n",
    "           'EuclideanDistance','ManhattanDistance','Day']\n",
    "    data_for_model_1 = data_for_model_1.drop(columns = var_drop, axis = 1)\n",
    "    data_for_model_1.rename(columns={\"REGION_N_dest\": \"destination_Region\"}, inplace = True)\n",
    "    data_for_model_1['Weather'].fillna('Not Available', inplace=True)\n",
    "    trip_Data=data_for_model_1['trj_id'].values\n",
    "    data_for_model_1_re=data_for_model_1[['destination_Region',  'popularity+time',  'timeOnly',  'dayOfWeek',  'hour',  'pathHIndex',  'pathaIndex',  'pathInDegreeIndex',  'pathOutDegreeIndex',  'CountOfGrid',  'destLoad',  'destHIndex',  'pickUpLoad',  'flowType',  'Festival',  'Temp',  'Weather',  'Wind',  'Humidity',  'Barometer',  'Visibility',  'Bus Terminal_pick',  'Business Area_pick',  'HDB_pick',  'Hospital_pick',  'LRT_pick',  'MRT_pick',  'Mall/Supermarket_pick',  'Other Residentials_pick',  'School/Kindergarten_pick',  'University/College_pick',  'Bus Terminal_des',  'Business Area_des',  'HDB_des',  'Hospital_des',  'LRT_des',  'MRT_des',  'Mall/Supermarket_des',  'Other Residentials_des',  'School/Kindergarten_des',  'University/College_des',  'rawlat_pickup',  'rawlng_pickup',  'rawlat_dest',  'rawlng_dest',  'HarvsineDistance',  'TimeInterval',  'PopularPlace',  'Weekend']]\n",
    "    join_predicted_1=pd.DataFrame()\n",
    "    if not data_for_model_1_re.empty:\n",
    "        data_for_model_1_re['eta']= loaded_model_1.predict(data_for_model_1_re)\n",
    "\n",
    "        predicted_values_1=data_for_model_1_re[['eta']]\n",
    "        predicted_values_1[\"trj_id\"]=trip_Data\n",
    "        join_predicted_1= pd.merge(df_original, predicted_values_1,on=\"trj_id\")\n",
    "\n",
    "    data_for_model_2=df_join3[(df_join3['popularity+time'].astype(str).apply(lambda x: len(x)) == 0)|(df_join3['timeOnlyPath'].astype(str).apply(lambda x: len(x)) == 0)]\n",
    "    if data_for_model_2.shape[0]>=1:\n",
    "        data_for_model_2['eta']= loaded_model_2.predict(data_for_model_2[[\"Festival\",'HarvsineDistance']])\n",
    "        predicted_values_2=data_for_model_2[['eta']]\n",
    "        predicted_values_2[\"trj_id\"]=data_for_model_2[\"trj_id\"].values\n",
    "        join_predicted_2= pd.merge(df_original, predicted_values_2,on=\"trj_id\")\n",
    "        finalprediction= join_predicted_1.append(join_predicted_2)\n",
    "\n",
    "    else: \n",
    "        finalprediction= join_predicted_1.copy()\n",
    "\n",
    "    return json.dumps(finalprediction.sort_values(\"trj_id\").drop(\"trj_id\",axis=1).eta.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_all = joblib.load( out + 'temp_all.pkl')\n",
    "temp_all = temp_all.reset_index(drop = True)\n",
    "sample = temp_all[['latitude_origin', 'longitude_origin', 'latitude_destination',\n",
    "                'longitude_destination', 'timestamp', 'hour_of_day', 'day_of_week']]\n",
    "target = temp_all[['eta']]\n",
    "id_    = temp_all[['trj_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversion to pandas\n",
    "def conv_(ch): \n",
    "    res = ch.strip('][').split(', ') \n",
    "    res = [s.strip('[\"]') for s in res]\n",
    "    res_pd = pd.DataFrame(res)\n",
    "    res_pd.columns = ['pred']\n",
    "    res_pd['pred'] = pd.to_numeric(res_pd['pred'])\n",
    "    return res_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\deepchannel\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n",
      "d:\\anaconda\\envs\\deepchannel\\lib\\site-packages\\geopandas\\tools\\sjoin.py:61: UserWarning: CRS of frames being joined does not match!(+init=epsg:4326 +type=crs != epsg:4326)\n",
      "  \"(%s != %s)\" % (left_df.crs, right_df.crs)\n",
      "d:\\anaconda\\envs\\deepchannel\\lib\\site-packages\\geopandas\\tools\\sjoin.py:61: UserWarning: CRS of frames being joined does not match!(+init=epsg:4326 +type=crs != epsg:4326)\n",
      "  \"(%s != %s)\" % (left_df.crs, right_df.crs)\n",
      "d:\\anaconda\\envs\\deepchannel\\lib\\site-packages\\pandas\\core\\frame.py:4133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "d:\\anaconda\\envs\\deepchannel\\lib\\site-packages\\ipykernel_launcher.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "ch_json = sample.iloc[0:5,:].to_json()\n",
    "pred = conv_(run(ch_json))\n",
    "# out_to_ch_all = pd.DataFrame()\n",
    "# pred_all = []\n",
    "# for i in range(len(ch)):\n",
    "#     ch_json = ch.loc[[i]].to_json()\n",
    "#     cols, out_to_ch, pred = run(ch_json)\n",
    "#     out_to_ch_all = pd.concat([out_to_ch_all,out_to_ch], axis = 0)\n",
    "#     pred_all = pred_all + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.17523716127701"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mean_squared_error(pred.pred, target.iloc[0:5,:])**0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
